{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8142b137",
   "metadata": {},
   "source": [
    "# Project Overview\n",
    "This notebook implements a BERT-based model to analyze financial news headlines and predict their effect on stock prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70f099aa-293c-4178-911c-90fab715a322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 22:34:12.664471: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749782052.680282 2431904 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749782052.685332 2431904 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749782052.697736 2431904 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749782052.697753 2431904 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749782052.697754 2431904 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749782052.697756 2431904 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-12 22:34:12.702200: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import optuna\n",
    "from datetime import datetime\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344e4311",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "This section covers data loading cleaning, tokenization, and label encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6ded61-8090-4e84-bd48-db72ac606c47",
   "metadata": {},
   "source": [
    "## How I Preprocessed the before Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85e961ee-a9ee-4e6e-ad0e-be5023c81b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentiment(contents_str):\n",
    "    try:\n",
    "        data = json.loads(contents_str)\n",
    "        if isinstance(data, list) and len(data) > 0 and 'sentiment' in data[0]:\n",
    "            return data[0]['sentiment']\n",
    "        return None\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bbf0879-0f54-4f44-9c70-ccb0f75313b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df['contents'].apply(extract_sentiment)\n",
    "\n",
    "label_map = {'neutral': 0, 'negative': -1, 'positive': 1}\n",
    "label_encoded_map = {'neutral': 1, 'positive': 2, 'negative': 0}\n",
    "\n",
    "df['label'] = df['sentiment'].map(label_map)\n",
    "df['label_encoded'] = df['sentiment'].map(label_encoded_map)\n",
    "\n",
    "df = df.dropna(subset=['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bac6416a-3a8d-4dd6-bf50-fb54a4750733",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['title'].astype(str) + ' ' + df['summary'].astype(str)\n",
    "df['label'] = df['label'].astype(int)\n",
    "df['label_encoded'] = df['label_encoded'].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84f5770-ba5b-4bdd-b1db-9dd1b5bde34d",
   "metadata": {},
   "source": [
    "## Loading the processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "583f7375-fddc-42a2-a9cf-8c6bba4f2cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset.csv\")\n",
    "df['date'] = pd.to_datetime(df['published_at']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b54cddc-18e6-47af-a81b-6f318c1816dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyjung24/anaconda3/envs/workenv/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-multilingual-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8842a722-908b-4513-ae01-2a3e72fa1cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=256)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d441bc6-9a44-4a5a-ac91-93ad39fa5b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['text'].tolist()\n",
    "labels = df['label_encoded'].tolist()\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = NewsDataset(train_texts, train_labels)\n",
    "val_dataset = NewsDataset(val_texts, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85d6b4d",
   "metadata": {},
   "source": [
    "# Model Hyperparameter Tuning\n",
    "This section covers how I computed hyperparameter tuning using optuna. I tuned number of train epochs, batch sizes, learning rate, weight decays based on the evaluation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8a48090-1c96-49f9-92f3-cb243713c0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = predictions.argmax(axis=1)\n",
    "    return {\"accuracy\": accuracy_score(labels, preds)}\n",
    "\n",
    "def model_init():\n",
    "    return BertForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "\n",
    "def objective(trial):\n",
    "    args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",\n",
    "        num_train_epochs=trial.suggest_int(\"num_train_epochs\", 2, 6),\n",
    "        per_device_train_batch_size=trial.suggest_categorical(\"batch_size\", [8, 16, 32]),\n",
    "        learning_rate=trial.suggest_float(\"learning_rate\", 2e-5, 5e-5, log=True),\n",
    "        weight_decay=trial.suggest_float(\"weight_decay\", 0.0, 0.3),\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=10,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model_init=model_init,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    result = trainer.train()\n",
    "    return trainer.evaluate()[\"eval_accuracy\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9a8c30-2e49-462c-aa8f-c40bed90610e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-12 22:38:28,304] A new study created in memory with name: no-name-67aa6451-8e2b-49ec-be53-1d4dc506da3a\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='197' max='560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [197/560 02:00 < 03:44, 1.62 it/s, Epoch 1.75/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.795900</td>\n",
       "      <td>0.749129</td>\n",
       "      <td>0.673743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b14dd47a-e4ed-4d1a-8d2d-432240c1fa5e",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "This section covers how I trained my BERT model based on the hyperparameter I found from prior section. Then, it shows the evaluation using the classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc2ad1a-96c3-457e-bcfe-9f19a87df828",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./best_model\",\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=4,\n",
    "    learning_rate=2.3897116952456707e-05,\n",
    "    weight_decay=0.21227675946613164,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    fp16 = True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb4c029-e8ea-431f-82c9-b2d18d24ba05",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c2078a-3d9a-40f2-995e-fa70df34d3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = trainer.predict(val_dataset)\n",
    "y_true = val_labels\n",
    "y_pred = preds.predictions.argmax(axis=1)\n",
    "\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7343eaff-1ab4-4217-ac05-3c8af8b87868",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./saved_model\")\n",
    "tokenizer.sae_pretrained(\"./saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32986498-94aa-4553-9043-647606e724bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aff6105-5178-4f33-9286-7b534752298f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce73f28-0a78-4bb9-a2c9-be0dc04b1d83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b602bf61-b548-4dc1-b591-59cc6616e840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7cf14a-de06-4d66-9740-25cc76aee511",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5a41b2-76de-4a33-b7b2-a1d6b554ff10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f00dea9-f1a5-4a6f-b00d-0b7151a848b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
